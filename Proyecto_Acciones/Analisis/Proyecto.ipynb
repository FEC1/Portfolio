{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scarping usando Python\n",
    "### Recuperando datos de acciones de la bolsa de valores\n",
    "usando requests y BeautifulSoup en www.groww.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para capturar los datos de las acciones de la bolsa de valores de la pagina web www.groww.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "def captura_datos():\n",
    "    if datetime.today().weekday() == 5 or datetime.datetime.today().weekday() == 6:\n",
    "        print(\"Fin de semana. No se ejecutará la captura de datos.\")\n",
    "        return  \n",
    "    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.1729.4'}\n",
    "    urls = [\n",
    "        'https://groww.in/us-stocks/nke',\n",
    "        'https://groww.in/us-stocks/ko',\n",
    "        'https://groww.in/us-stocks/msft',  \n",
    "        'https://groww.in/us-stocks/axp', \n",
    "        'https://groww.in/us-stocks/amgn', \n",
    "        'https://groww.in/us-stocks/aapl', \n",
    "        'https://groww.in/us-stocks/ba', \n",
    "        'https://groww.in/us-stocks/csco', \n",
    "        'https://groww.in/us-stocks/gs', \n",
    "        'https://groww.in/us-stocks/ibm', \n",
    "        'https://groww.in/us-stocks/intc', \n",
    "        'https://groww.in/us-stocks/jpm', \n",
    "        'https://groww.in/us-stocks/mcd',\n",
    "        'https://groww.in/us-stocks/crm', \n",
    "        'https://groww.in/us-stocks/vz', \n",
    "        'https://groww.in/us-stocks/v', \n",
    "        'https://groww.in/us-stocks/wmt',  \n",
    "        'https://groww.in/us-stocks/dis' \n",
    "        \n",
    "        ]\n",
    "    datos_crudos = []\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Obteniendo datos de: {url}\")\n",
    "\n",
    "        try:\n",
    "        \n",
    "            respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "        \n",
    "            if respuesta.status_code != 200:\n",
    "                print(f\"Fallo el fetch {url} (Status: {respuesta.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "            \n",
    "            empresa = soup.find('h1', {'class': 'usph14Head displaySmall'})\n",
    "            nombre_empresa = empresa.get_text(strip=True) if empresa else \"N/A\"\n",
    "\n",
    "            precio = soup.find('span', {'class': 'uht141Pri contentPrimary displayBase'})\n",
    "            precio_accion = precio.get_text(strip=True) if precio else \"N/A\"\n",
    "\n",
    "            cambio = soup.find('div', {'class': ['uht141Day bodyBaseHeavy contentNegative','uht141Day bodyBaseHeavy contentPositive']})\n",
    "            cambio_accion = cambio.get_text(strip=True) if cambio else \"N/A\"\n",
    "\n",
    "            tabla_volumen = soup.find('table', {'class': 'tb10Table borderPrimary width100 usp100NoBorder usp100Table'})\n",
    "            if tabla_volumen:\n",
    "                filas = tabla_volumen.find_all('tr')  \n",
    "                if len(filas) > 1:  \n",
    "                    celdas = filas[1].find_all('td')  \n",
    "                    if len(celdas) > 2:  \n",
    "                        volumen_accion = celdas[2].get_text(strip=True)  \n",
    "                    else:\n",
    "                        volumen_accion = \"N/A\"\n",
    "                else:\n",
    "                    volumen_accion = \"N/A\"\n",
    "            else:\n",
    "                volumen_accion = \"N/A\"\n",
    "            \n",
    "            datos_crudos.append([nombre_empresa, precio_accion, cambio_accion, volumen_accion])\n",
    "\n",
    "            print(f\"✔ Fetch exitoso para: {nombre_empresa}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error del fetch {url}: {str(e)}\")\n",
    "\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(datos_crudos, columns=['Empresa', 'Precio', 'Cambio', 'Volumen'])\n",
    "    df['Precio'] = df['Precio'].str.replace('$', '', regex=False).astype(float)\n",
    "    def extraer_cambio(cadena):\n",
    "    \n",
    "        m = re.match(r'([+-]?\\d+\\.?\\d*)\\(([\\d\\.]+)%\\)', cadena)\n",
    "        if m:\n",
    "            cambio_val = m.group(1)   \n",
    "            cambio_pct = m.group(2)   \n",
    "            return pd.Series([float(cambio_val), cambio_pct])\n",
    "        else:\n",
    "            return pd.Series([None, None])\n",
    "    df[['Cambio_Valor', 'Cambio_Porcentaje']] = df['Cambio'].fillna('').apply(extraer_cambio)\n",
    "    df.drop(columns=['Cambio'], inplace=True)\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    df.to_csv(f'Acciones_{timestamp}.csv', index=False, encoding='utf-8-sig')\n",
    "def captura_datos_oro():\n",
    "    if datetime.today().weekday() == 5 or datetime.datetime.today().weekday() == 6:\n",
    "        print(\"Fin de semana. No se ejecutará la captura de datos.\")\n",
    "        return  \n",
    "    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.1729.4'}\n",
    "    urls = [\n",
    "        'https://es.investing.com/currencies/xau-usd'\n",
    "            \n",
    "        ]\n",
    "    datos_oro = []\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Obteniendo datos de: {url}\")\n",
    "\n",
    "        try:\n",
    "        \n",
    "            respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "        \n",
    "            if respuesta.status_code != 200:\n",
    "                print(f\"Fallo el fetch {url} (Status: {respuesta.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "            def extraer_valor(data_test):\n",
    "                elemento = soup.find('dd', {'data-test': data_test})\n",
    "                if elemento:\n",
    "                    span = elemento.find('span', {'class': 'key-info_dd-numeric__ZQFIs'})\n",
    "                    if span:\n",
    "                        spans = span.find_all('span')\n",
    "                        return spans[1].get_text(strip=True) if len(spans) > 1 else \"N/A\"\n",
    "                return \"N/A\"\n",
    "        \n",
    "            ultimo_cierre = extraer_valor(\"prevClose\")\n",
    "            apertura = extraer_valor(\"open\")\n",
    "            valor_compra = extraer_valor(\"bid\")\n",
    "            datos_oro.append([ultimo_cierre, valor_compra, apertura])\n",
    "\n",
    "            print(f\"✔ Fetch exitoso para oro\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error del fetch {url}: {str(e)}\")\n",
    "    df = pd.DataFrame(datos_oro, columns=[ \"Último Cierre\", \"Apertura\", \"Valor de Compra\"])\n",
    "\n",
    "    # Mostrar el DataFrame\n",
    "    print(\"\\n Datos extraídos:\")\n",
    "    print(df)\n",
    "    fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    df.to_csv(f\"oro_{fecha_actual}.csv\", index=False, encoding=\"utf-8\")\n",
    "schedule.every().day.at(\"15:30\").do(captura_datos) \n",
    "schedule.every().day.at('15:33').do(captura_datos_oro) \n",
    "schedule.every().day.at(\"17:57\").do(captura_datos)  \n",
    "schedule.every().day.at(\"21:27\").do(captura_datos)   \n",
    "schedule.every().day.at(\"21:57\").do(captura_datos)  \n",
    "print(\"Scheduler iniciado. Esperando próximos horarios de captura...\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    proximo_evento = schedule.next_run()\n",
    "    tiempo_espera = (proximo_evento - datetime.now()).total_seconds()\n",
    "        \n",
    "    if proximo_evento:\n",
    "        tiempo_espera = (proximo_evento - datetime.now()).total_seconds()\n",
    "    else:\n",
    "        ahora = datetime.now()\n",
    "        primer_evento_mañana = ahora.replace(hour=15, minute=30, second=0, microsecond=0) + timedelta(days=1) \n",
    "        tiempo_espera = (primer_evento_mañana - ahora).total_seconds()\n",
    "\n",
    "    print(f\"Esperando {tiempo_espera:.0f} segundos hasta el próximo evento.\")\n",
    "    time.sleep(max(tiempo_espera-60,18000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para capturar datos de oro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo datos de: https://es.investing.com/currencies/xau-usd\n",
      "✔ Fetch exitoso para oro\n",
      "\n",
      " Datos extraídos:\n",
      "  Último Cierre  Apertura Valor de Compra\n",
      "0      3.019,28  3.052,99        3.012,28\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "def captura_datos_oro():\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.1729.4'}\n",
    "    urls = [\n",
    "        'https://es.investing.com/currencies/xau-usd'\n",
    "            \n",
    "        ]\n",
    "    datos_oro = []\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Obteniendo datos de: {url}\")\n",
    "\n",
    "        try:\n",
    "        \n",
    "            respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "        \n",
    "            if respuesta.status_code != 200:\n",
    "                print(f\"Fallo el fetch {url} (Status: {respuesta.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "            def extraer_valor(data_test):\n",
    "                elemento = soup.find('dd', {'data-test': data_test})\n",
    "                if elemento:\n",
    "                    span = elemento.find('span', {'class': 'key-info_dd-numeric__ZQFIs'})\n",
    "                    if span:\n",
    "                        spans = span.find_all('span')\n",
    "                        return spans[1].get_text(strip=True) if len(spans) > 1 else \"N/A\"\n",
    "                return \"N/A\"\n",
    "        \n",
    "            ultimo_cierre = extraer_valor(\"prevClose\")\n",
    "            apertura = extraer_valor(\"open\")\n",
    "            valor_compra = extraer_valor(\"bid\")\n",
    "            datos_oro.append([ultimo_cierre, valor_compra, apertura])\n",
    "\n",
    "            print(f\"✔ Fetch exitoso para oro\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error del fetch {url}: {str(e)}\")\n",
    "    df = pd.DataFrame(datos_oro, columns=[ \"Último Cierre\", \"Apertura\", \"Valor de Compra\"])\n",
    "\n",
    "    # Mostrar el DataFrame\n",
    "    print(\"\\n Datos extraídos:\")\n",
    "    print(df)\n",
    "    fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    df.to_csv(f\"oro_{fecha_actual}.csv\", index=False, encoding=\"utf-8\")\n",
    "captura_datos_oro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programacion del horario de captura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo datos de: https://groww.in/us-stocks/nke\n",
      "✔ Fetch exitoso para: Nike Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/ko\n",
      "✔ Fetch exitoso para: Coca-Cola Company The\n",
      "Obteniendo datos de: https://groww.in/us-stocks/msft\n",
      "✔ Fetch exitoso para: Microsoft Corporation\n",
      "Obteniendo datos de: https://groww.in/us-stocks/axp\n",
      "✔ Fetch exitoso para: American Express Co\n",
      "Obteniendo datos de: https://groww.in/us-stocks/amgn\n",
      "✔ Fetch exitoso para: Amgen Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/aapl\n",
      "✔ Fetch exitoso para: Apple Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/ba\n",
      "✔ Fetch exitoso para: Boeing Company The\n",
      "Obteniendo datos de: https://groww.in/us-stocks/csco\n",
      "✔ Fetch exitoso para: Cisco Systems Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/gs\n",
      "✔ Fetch exitoso para: Goldman Sachs Group Inc The\n",
      "Obteniendo datos de: https://groww.in/us-stocks/ibm\n",
      "✔ Fetch exitoso para: International Business Machines Corp\n",
      "Obteniendo datos de: https://groww.in/us-stocks/intc\n",
      "✔ Fetch exitoso para: Intel Corporation\n",
      "Obteniendo datos de: https://groww.in/us-stocks/jpm\n",
      "✔ Fetch exitoso para: JPMorgan Chase & Co\n",
      "Obteniendo datos de: https://groww.in/us-stocks/mcd\n",
      "✔ Fetch exitoso para: McDonald's Corp\n",
      "Obteniendo datos de: https://groww.in/us-stocks/crm\n",
      "✔ Fetch exitoso para: Salesforcecom Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/vz\n",
      "✔ Fetch exitoso para: Verizon Communications Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/v\n",
      "✔ Fetch exitoso para: Visa Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/wmt\n",
      "✔ Fetch exitoso para: Walmart Inc\n",
      "Obteniendo datos de: https://groww.in/us-stocks/dis\n",
      "✔ Fetch exitoso para: Walt Disney Company The\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "def captura_datos():\n",
    "    if datetime.today().weekday() == 5 or datetime.today().weekday() == 6:\n",
    "        print(\"Fin de semana. No se ejecutará la captura de datos.\")\n",
    "        return  \n",
    "    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.1729.4'}\n",
    "    urls = [\n",
    "        'https://groww.in/us-stocks/nke',\n",
    "        'https://groww.in/us-stocks/ko',\n",
    "        'https://groww.in/us-stocks/msft',  \n",
    "        'https://groww.in/us-stocks/axp', \n",
    "        'https://groww.in/us-stocks/amgn', \n",
    "        'https://groww.in/us-stocks/aapl', \n",
    "        'https://groww.in/us-stocks/ba', \n",
    "        'https://groww.in/us-stocks/csco', \n",
    "        'https://groww.in/us-stocks/gs', \n",
    "        'https://groww.in/us-stocks/ibm', \n",
    "        'https://groww.in/us-stocks/intc', \n",
    "        'https://groww.in/us-stocks/jpm', \n",
    "        'https://groww.in/us-stocks/mcd',\n",
    "        'https://groww.in/us-stocks/crm', \n",
    "        'https://groww.in/us-stocks/vz', \n",
    "        'https://groww.in/us-stocks/v', \n",
    "        'https://groww.in/us-stocks/wmt',  \n",
    "        'https://groww.in/us-stocks/dis' \n",
    "        \n",
    "        ]\n",
    "    datos_crudos = []\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Obteniendo datos de: {url}\")\n",
    "\n",
    "        try:\n",
    "        \n",
    "            respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "        \n",
    "            if respuesta.status_code != 200:\n",
    "                print(f\"Fallo el fetch {url} (Status: {respuesta.status_code})\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "            \n",
    "            empresa = soup.find('h1', {'class': 'usph14Head displaySmall'})\n",
    "            nombre_empresa = empresa.get_text(strip=True) if empresa else \"N/A\"\n",
    "\n",
    "            precio = soup.find('span', {'class': 'uht141Pri contentPrimary displayBase'})\n",
    "            precio_accion = precio.get_text(strip=True) if precio else \"N/A\"\n",
    "\n",
    "            cambio = soup.find('div', {'class': ['uht141Day bodyBaseHeavy contentNegative','uht141Day bodyBaseHeavy contentPositive']})\n",
    "            cambio_accion = cambio.get_text(strip=True) if cambio else \"N/A\"\n",
    "\n",
    "            tabla_volumen = soup.find('table', {'class': 'tb10Table borderPrimary width100 usp100NoBorder usp100Table'})\n",
    "            if tabla_volumen:\n",
    "                filas = tabla_volumen.find_all('tr')  \n",
    "                if len(filas) > 1:  \n",
    "                    celdas = filas[1].find_all('td')  \n",
    "                    if len(celdas) > 2:  \n",
    "                        volumen_accion = celdas[2].get_text(strip=True)  \n",
    "                    else:\n",
    "                        volumen_accion = \"N/A\"\n",
    "                else:\n",
    "                    volumen_accion = \"N/A\"\n",
    "            else:\n",
    "                volumen_accion = \"N/A\"\n",
    "            \n",
    "            datos_crudos.append([nombre_empresa, precio_accion, cambio_accion, volumen_accion])\n",
    "\n",
    "            print(f\"✔ Fetch exitoso para: {nombre_empresa}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error del fetch {url}: {str(e)}\")\n",
    "\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(datos_crudos, columns=['Empresa', 'Precio', 'Cambio', 'Volumen'])\n",
    "    df['Precio'] = df['Precio'].str.replace('$', '', regex=False).astype(float)\n",
    "    def extraer_cambio(cadena):\n",
    "    \n",
    "        m = re.match(r'([+-]?\\d+\\.?\\d*)\\(([\\d\\.]+)%\\)', cadena)\n",
    "        if m:\n",
    "            cambio_val = m.group(1)   \n",
    "            cambio_pct = m.group(2)   \n",
    "            return pd.Series([float(cambio_val), cambio_pct])\n",
    "        else:\n",
    "            return pd.Series([None, None])\n",
    "    df[['Cambio_Valor', 'Cambio_Porcentaje']] = df['Cambio'].fillna('').apply(extraer_cambio)\n",
    "    df.drop(columns=['Cambio'], inplace=True)\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    df.to_csv(f'Acciones_{timestamp}.csv', index=False, encoding='utf-8-sig')\n",
    "captura_datos() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
